version = 1

; (Optional) This section provides global settings shared across all presets.
; If the same key is defined in a specific preset, it will override the value in this global section.
[*]
n-gpu-layers = -1
ctx-size = 0
mmap = true
prio = 3
no-context-shift = true
context-shift = 0
no-kv-offload = true
no-warmup = true
batch-size = 4096
ubatch-size = 1024
flash-attn = true
jinja = true
threads = 16
log-verbosity = 3
no-slots = false
parallel = 4
; Set to -1 to ensure the GPU sleeps when not in use
sleep-idle-seconds = -1
; Use 8-bit KV Cache to save massive RAM for 96GB setup
cache-type-k = q8_0
cache-type-v = q8_0
kv-unified = true
spec-type = ngram-map-k
chat-template = chatml

draft-max = 64
draft-min = 5

temp = 0.2          ; Lowered for coding to prevent logic drift
repeat-penalty = 1.1 ; Increased slightly to prevent loops
top-p = 0.95        ; Standard for Qwen
min-p = 0.05        ; Better than top-k for preventing gibberish

[extra.gpt-oss-120b-GGUF]
model = ggml-org/gpt-oss-120b-GGUF/gpt-oss-120b-mxfp4-00001-of-00003.gguf
n-predict = 262144
repeat-penalty = 1.05
reasoning-format = none

[extra.claude-fast]
model = unsloth/Qwen3-30B-A3B-Thinking-2507/Qwen3-30B-A3B-Thinking-2507-Q8_0.gguf

[extra.claude-deep]
model = unsloth/Qwen3-30B-A3B-Thinking-2507/Qwen3-30B-A3B-Thinking-2507-Q8_0.gguf

[opencode.thinking]
model = unsloth/Qwen3-30B-A3B-Thinking-2507/Qwen3-30B-A3B-Thinking-2507-Q8_0.gguf

[opencode.instruct]
model = unsloth/Qwen3-30B-A3B-Instruct-2507/Qwen3-30B-A3B-Instruct-2507-Q8_0.gguf

[opencode.coder]
model = unsloth/Qwen3-Coder-30B-A3B-Instruct/Qwen3-Coder-30B-A3B-Instruct-Q8_0.gguf

[opencode.gemma]
model = unsloth/gemma-3-1b-it/gemma-3-1b-it-Q8_0.gguf
