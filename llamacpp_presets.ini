version = 1

; (Optional) This section provides global settings shared across all presets.
; If the same key is defined in a specific preset, it will override the value in this global section.
[*]
n-gpu-layers = -1
ctx-size = 0
mmap = true
prio = 3
no-context-shift = true
context-shift = 0
no-kv-offload = true
no-warmup = true
batch-size = 4096
ubatch-size = 1024
flash-attn = true
jinja = true
threads = 16
log-verbosity = 3
no-slots = false
parallel = 4
; Set to -1 to ensure the GPU sleeps when not in use
sleep-idle-seconds = -1
; Use 8-bit KV Cache to save massive RAM for 96GB setup
cache-type-k = q8_0
cache-type-v = q8_0
kv-unified = true
spec-type = ngram-map-k
chat-template = chatml

draft-max = 64
draft-min = 5

[extra.gpt-oss-120b-GGUF]
model = /models/ggml-org/gpt-oss-120b-GGUF/gpt-oss-120b-mxfp4-00001-of-00003.gguf
n-predict = 262144
repeat-penalty = 1.05
reasoning-format = none

[extra.claude-fast]
; model = /models/TeichAI/glm-4.7-flash-claude-4.5-opus.q8_0.gguf
; model = /models/unsloth/GLM-4.7-Flash/GLM-4.7-Flash-Q8_0.gguf
;model = /models/ggml-org/gpt-oss-120b-GGUF/gpt-oss-120b-mxfp4-00001-of-00003.gguf
; model = /models/unsloth/Llama-3.2-1B-Instruct/Llama-3.2-1B-Instruct-UD-Q4_K_XL.gguf
model = /models/unsloth/Qwen3-30B-A3B-Thinking-2507/Qwen3-30B-A3B-Thinking-2507-Q8_0.gguf

[extra.claude-deep]
; model = /models/ggml-org/gpt-oss-120b-GGUF/gpt-oss-120b-mxfp4-00001-of-00003.gguf
; model = /models/TeichAI/glm-4.7-flash-claude-4.5-opus.q8_0.gguf
; model = /models/unsloth/GLM-4.7-Flash/GLM-4.7-Flash-Q8_0.gguf
; model = /models/ggml-org/gpt-oss-120b-GGUF/gpt-oss-120b-mxfp4-00001-of-00003.gguf
; model = /models/unsloth/Llama-3.2-1B-Instruct/Llama-3.2-1B-Instruct-UD-Q4_K_XL.gguf
model = /models/unsloth/Qwen3-30B-A3B-Thinking-2507/Qwen3-30B-A3B-Thinking-2507-Q8_0.gguf

[opencode]
model = /models/unsloth/Qwen3-30B-A3B-Thinking-2507/Qwen3-30B-A3B-Thinking-2507-Q8_0.gguf
